{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network\n",
    "\n",
    "This projects deals with building an Artificial Neural Network and understanding why Neural Networks outperform most of the other simpler Machine Learning Algorithms.\n",
    "\n",
    "The goal of ANN is to build a network of neurons with 1 or more hidden layers of neurons to allow for backpropagation of synapses that improves the performance/accuracy of the model. The goal of this type of models is to constantly keep checking for loss function which is the difference between predicted y_hat and actual Y to adjust the weights at the input nodes and hidden nodes.\n",
    "\n",
    "Lets start with importing the necessary modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install the KERAS library, Tensorflow and Theano done using the below commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: keras in c:\\users\\rames\\anaconda3\\lib\\site-packages\n",
      "Requirement already up-to-date: pyyaml in c:\\users\\rames\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already up-to-date: scipy>=0.14 in c:\\users\\rames\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already up-to-date: six>=1.9.0 in c:\\users\\rames\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already up-to-date: numpy>=1.9.1 in c:\\users\\rames\\anaconda3\\lib\\site-packages (from keras)\n",
      "Collecting git+git://github.com/Theano/Theano.git\n",
      "  Cloning git://github.com/Theano/Theano.git to c:\\users\\rames\\appdata\\local\\temp\\pip-11cdfxm6-build\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Error [WinError 2] The system cannot find the file specified while executing command git clone -q git://github.com/Theano/Theano.git C:\\Users\\rames\\AppData\\Local\\Temp\\pip-11cdfxm6-build\n",
      "Cannot find command 'git'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\rames\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\rames\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\rames\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: tensorflow-tensorboard<0.2.0,>=0.1.0 in c:\\users\\rames\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: protobuf>=3.3.0 in c:\\users\\rames\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\rames\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: bleach==1.5.0 in c:\\users\\rames\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Requirement already satisfied: html5lib==0.9999999 in c:\\users\\rames\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rames\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\users\\rames\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rames\\anaconda3\\lib\\site-packages (from protobuf>=3.3.0->tensorflow)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade keras\n",
    "!pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data and performing some preprocessing to clean the data\n",
    "\n",
    "The data that we would be using is a sample of a fictional dataset. The data fictiously represents a bank data with geo demographical information of the customers such as \n",
    "1. customerId, \n",
    "2. Last Name, \n",
    "3. CreditScore, \n",
    "4. Geography( Here France, Spain and Germany), \n",
    "5. Gender, \n",
    "6. Age, \n",
    "7. Tenure, \n",
    "8. Current Balance in accounts, \n",
    "9. Number of Products the customer currently has, \n",
    "10. Whether he has credit card with the bank, \n",
    "11. Is he an active customer (last 2 months), \n",
    "12. Estimated Salary\n",
    "\n",
    "Finally the dependent variable \n",
    "Exited - 1 denoting he stopped being a customer and 0 representing still a customer\n",
    "\n",
    "A copy of the csv is available in here:(Add link here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets import the data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split the data into input nodes and output nodes.\n",
    "X represent the input nodes. \n",
    "Y represent the output nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 3:13].values\n",
    "Y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Categorical data\n",
    "\n",
    "Since out input data has two categorical data in Geography and Gender, we need to convert them into numerical fields. We can use One Hot Encoder to do that and then remove one of the Geography encoded values to avoid Dummy Variable Trap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "X=X[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the available data into Test and Train\n",
    "\n",
    "We can directly use train test split from the sklearn to split 20% of the data as test data for our evaluations later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Since our Output node is a binary outcome, we will be using the sigmoid/logistic activation function in our output node. Therefore it is essential for us to standardize the independent variables to allow for the sigmoid function to capture the variance better. Here we are going to do that using StandardScalar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Artificial Neural Network\n",
    "\n",
    "Now we can start building an artificial neural Network\n",
    "\n",
    "Step one is to import all the necessary modules from the Keras library. We are importing Sequential and Dense \n",
    "\n",
    "Step 2 is to initiate a Neural Network which is done using Sequential function. This signifies that our neural nodes would be sequentially arranged\n",
    "\n",
    "Step 3 is to initiate our first hidden layer\n",
    "* We use the add function to add our first hidden layer\n",
    "\n",
    "* Here we need to specify the number of output dimensions, which would be the number of nodes in our first hidden layer. As a general rule of thumb, it is acceptable to use the average of input and output nodes as \"the\" number of nodes here. Since we have 1 output node and 11 input nodes, we use an average of those two numbers to determine the number of nodes in our first hidden layer\n",
    "\n",
    "* The next thing we need to specify is the initialization function . Uniform allows us to associate a small non zero value to the initial weights for the input.\n",
    "\n",
    "* I am adding a rectifier function as activation function for our first hidden layer\n",
    "\n",
    "* Finally the input dimentions in our case =11\n",
    "\n",
    "Step 4 is to initialize the second hidden layer\n",
    "* Using the same number of nodes as the first layer\n",
    "* Using the same rectifier activation function\n",
    "\n",
    "\n",
    "Step 5 - Define the output Node\n",
    "* Our Output node has 1 node since this is a binary output of 1 or 0\n",
    "* Our activation function, of course, is a sigmoid function since it gives a nice probability that can be used to convert to 0 or 1 using a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the keras libraries to create out Neural Networks\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the initial object as a sequential model with more than one layers\n",
    "clf=Sequential()\n",
    "\n",
    "# Adding the first hidden layer\n",
    "# output_dim is the average of input dims=11 and output dims=1.\n",
    "# init is the function to imitialize the weights. uniform is the most simple way to add a close to 0 value for weights \n",
    "# Choosing the activation function to be relu which corresponds to rectifier function \n",
    "clf.add(Dense(units=6,kernel_initializer=\"uniform\",activation='relu',input_dim=11))\n",
    "\n",
    "#Adding the second hidden layer\n",
    "clf.add(Dense(units=6,kernel_initializer=\"uniform\",activation='relu'))\n",
    "\n",
    "# Adding the final output layer\n",
    "clf.add(Dense(units=1,kernel_initializer=\"uniform\",activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the whole ANN.. \n",
    "\n",
    "* Applying Stochastic Neural Network as our optimizer which is the optimizing algorithm that is used for weight calculation\n",
    "* So far the weights have only been initialized, so the Stochastic algorithm would be used for weight estimation/calculation . Done here using ADAM\n",
    "* Loss represents the loss function that will used to find the optimized solution\n",
    "* Metrics are the criterion that are used to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4819 - acc: 0.7959     \n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4286 - acc: 0.7960     \n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4231 - acc: 0.7960     \n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4191 - acc: 0.8199     \n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4162 - acc: 0.8292     \n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4143 - acc: 0.8304     \n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4129 - acc: 0.8336     \n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4118 - acc: 0.8320     \n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4108 - acc: 0.8336     \n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4096 - acc: 0.8339     \n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4087 - acc: 0.8345     \n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4085 - acc: 0.8352     \n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4073 - acc: 0.8342     \n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4074 - acc: 0.8350     \n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4065 - acc: 0.8347     \n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4059 - acc: 0.8356     \n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4062 - acc: 0.8342     \n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4058 - acc: 0.8355     \n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4049 - acc: 0.8324     \n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.4033 - acc: 0.836 - 0s - loss: 0.4050 - acc: 0.8355     \n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4045 - acc: 0.8345     \n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4047 - acc: 0.8341     \n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4042 - acc: 0.8340     \n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4037 - acc: 0.8337     \n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4039 - acc: 0.8339     \n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4035 - acc: 0.8354     \n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4040 - acc: 0.8349     \n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4032 - acc: 0.8340     \n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4033 - acc: 0.8357     \n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4027 - acc: 0.8332     \n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4026 - acc: 0.8365     \n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4022 - acc: 0.8347     \n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4024 - acc: 0.8364     \n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4023 - acc: 0.8347     \n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4020 - acc: 0.8349     \n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4015 - acc: 0.8346     \n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4019 - acc: 0.8351     \n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4014 - acc: 0.8346     \n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4016 - acc: 0.8342     \n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4018 - acc: 0.8341     \n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4015 - acc: 0.8335     \n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4017 - acc: 0.8341     \n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4014 - acc: 0.8345     \n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4015 - acc: 0.8349     \n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4013 - acc: 0.8335     \n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4016 - acc: 0.8350     \n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4010 - acc: 0.8340     \n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4013 - acc: 0.8351     \n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4014 - acc: 0.8349     \n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4010 - acc: 0.8351     \n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4007 - acc: 0.8354     - ETA: 0s - loss: 0.4036 - acc: 0.8\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4010 - acc: 0.8354     \n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4005 - acc: 0.8375     \n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4009 - acc: 0.8346     \n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4011 - acc: 0.8332     \n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4011 - acc: 0.8340     \n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4006 - acc: 0.8334     \n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4010 - acc: 0.8344     \n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4006 - acc: 0.8341     \n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4009 - acc: 0.8336     \n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4005 - acc: 0.8340     \n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4005 - acc: 0.8351     \n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4005 - acc: 0.8355     \n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4007 - acc: 0.8352     \n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4004 - acc: 0.8339     \n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4008 - acc: 0.8376     \n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4005 - acc: 0.8350     \n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4009 - acc: 0.8341     \n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4002 - acc: 0.8346     \n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4006 - acc: 0.8346     \n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4005 - acc: 0.8354     \n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4002 - acc: 0.8346     \n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4003 - acc: 0.8350     \n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4002 - acc: 0.8344     \n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4002 - acc: 0.8344     \n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4004 - acc: 0.8347     \n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4007 - acc: 0.8339     \n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4003 - acc: 0.8347     \n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4006 - acc: 0.8357     \n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4004 - acc: 0.8365     \n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4001 - acc: 0.8352     \n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4008 - acc: 0.8349     \n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4001 - acc: 0.8351     \n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4006 - acc: 0.8350     \n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3997 - acc: 0.8345     \n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s - loss: 0.4003 - acc: 0.8335     \n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4002 - acc: 0.8344     \n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3999 - acc: 0.8355     \n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4008 - acc: 0.8339     \n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4004 - acc: 0.8346     \n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4006 - acc: 0.8352     \n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4001 - acc: 0.8351     \n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3998 - acc: 0.8347     \n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4004 - acc: 0.8356     \n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4000 - acc: 0.8354     \n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4007 - acc: 0.8352     \n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4001 - acc: 0.8331     \n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4001 - acc: 0.8344     \n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4002 - acc: 0.8356     \n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4007 - acc: 0.8349     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f6c71cd710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction and model evaluation\n",
    "\n",
    "clf.fit(X_train,Y_train,batch_size=10,epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1548,   47],\n",
       "       [ 272,  133]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the model on the split test data\n",
    "Y_pred=clf.predict(X_test)\n",
    "Y_pred=(Y_pred>0.5)\n",
    "\n",
    "# Calculating the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(Y_test,Y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above results we can infer that the model consistently predicts with an accuracy of about 83% in the train dataset. From the confusion Matrix we can calcualte the accuracy in the test data as (1548+133)/2000 = 84%.\n",
    "\n",
    "Based on the above the model is pretty robust with consistent accuracy metrics leading to believe that there is no over fitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
